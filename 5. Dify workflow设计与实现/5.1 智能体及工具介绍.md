# 5.1 智能体及工具介绍

## 智能体概念介绍

什么是 Agent ？

>Agent（智能体） = 一个设置了一些目标或任务，可以迭代运行的大型语言模型。这与大型语言模型（LLM）在像ChatGPT这样的工具中“通常”的使用方式不同。在ChatGPT中，你提出一个问题并获得一个答案作为回应。而Agent拥有复杂的工作流程，模型本质上可以自我对话，而无需人类驱动每一部分的交互。                                                                                                                        -Logan Kilpatrick，OpenAI 开发者关系负责人

### 从强化学习开始学习 Agent    
强化学习（Reinforcement Learning）是一种机器学习方法，它旨在让一个智能体（Agent）通过与环境的交互来学习如何做出决策，以最大化累积奖励或实现特定目标。 

为什么要先介绍RL中的Agent呢？  

+ 一方面是大模型的训练离不开RL的帮助，而基于大模型的Agent与RL也存在关联  

+ 另一方面是我觉得<red>目前LLM的Agent概念与RL中的Agent存在很强的相关性</red>，所以需要讲讲

强化学习中有这样一些概念：

* **Agent（智能体）**：  
Agent 是强化学习中的主体，它代表了学习决策的实体。Agent的目标是通过与环境互动，学会选择行动来实现其预定的任务或最大化累积奖励。在RL算法中我们会用Q表，神经网络等来实现这个Agent。  

* **环境(env)**：  
环境是Agent操作的背景，它包括Agent可以感知和影响的所有事物。环境可以是现实世界中的物理环境，也可以是虚拟的仿真环境。Agent与环境之间的互动通常通过观测、执行动作和接收奖励来实现。  

* **状态（State）**：  
状态是环境的一个描述，它包含了环境的所有关键信息，以便Agent能够做出决策。状态可以是离散的或连续的，具体取决于具体问题。

* **动作（Action）**：  
动作是Agent在环境中执行的操作或决策。Agent需要选择动作来影响环境，以实现其目标或最大化奖励。  

* **奖励（Reward）**：  
奖励是一个数值信号，用于评估Agent的行动。奖励通常定义了任务的目标，即Agent应该努力最大化的数量。Agent的目标是通过选择动作来最大化预期累积奖励。

* **策略（Policy）**：  
策略定义了Agent在给定状态下选择动作的方式。它可以是确定性策略，也可以是随机策略。目标是找到一个最优策略，以使Agent能够在不同状态下最大化累积奖励。  

* **价值函数（Value Function）**：  
价值函数用于评估状态或状态-动作对的价值，即在特定状态下或采取特定动作后，Agent可以望获得多少累积奖励。值函数有两种类型：状态值函数（V函数）和动作值函数（Q函数）。


对于强化学习，我们需要让一个神经网络（或者说智能体）在**特定的决策空间**中找出一条奖励最高的路径，我们把这样一个探索+学习后得到的神经网络就成为强化学习中的Agent。  
  

这个Agent它应该拥有拥有感知环境的能力（与环境交互获得反馈），自主决策（在有限的决策空间内），长远来看还有规划行动的能力，也就是它会学习出一个最优的policy。

### LLM出现后的Agent

在大模型出现后我们谈论 Agent 时我们更多谈论的是基于 LLM 也就是大规模语言模型的 Agent ，前面我们已经介绍过了 Agent 将要实现的功能，那么我们怎样才能让他做到这些事情呢？

一个相关的 Agent 范式是

> Agent = LLMS + Memory + Planning + Feedback + Tools 

感知（Perception）→ 规划（Planning）→ 行动（Action）→ 获得反馈（Feedback）  

+ 感知（Perception）是指Agent从环境中收集信息并从中提取相关知识的能力。  （对于大语言模型就意味着如何去管理大模型接受的上下文）

+ 规划（Planning）是指Agent为了某一目标而作出的决策过程。  

+ 行动（Action）是指基于环境和规划做出的动作。  

+ 获得反馈（Feedback）是指Agent应该能够感知到自己决策的结果  


有一个很有意思的比喻是  

我们把LLM也就是大语言模型本省比作一个疯狂原始人  

它可能有一些智能,因为它掌握了基本的生存能力，会做简单的算术，但是还远远谈不到聪明，它没有办法完成基础之上更复杂的任务

怎么让这个LLM变聪明呢？  

就像原始人发展到现代社会一样，我们的祖先学会使用火（Tools），接着学会把这项经验传递（Memory），现在这个LLM已经能完成非常多本来不能够完成的任务了  

那能不能让Agent变得更聪明？

一种方法是，我们可以对LLM做特定任务上的微调  

我们把让LLM在它原有的基础上再去获取一些垂直领域的知识  

另一种方法是，我们为他增加额外的RAG，也就是知识库，来提升它的能力

有一些任务是他本身简单思考就能够完成的，比如帮我总结某个网页内容，观察某段代码的运行结果  

还有一些任务，我们可以像父母引导孩子一样（第一次走路？）引导他以一种step by step 的方式  

> 把大象装进冰箱需要几步？  

现在进化为使用CoT，ToT，GoT等方法来完成这个目标，而这些方法都基于**通用问题求解器，也就是GPS思想**的指导  

将一些复杂的任务一步一步的简单化后，这些子问题已经能够被LLM自己解决了  

比如这个原始人可能只能搞定10以内的加法，但是其实像9999989899+45464113134564这类复杂的问题也可以被拆成一个一个10以内的加法来完成。  

通过高效的组织架构，我们可以让不那么聪明（或者说还在持续进化）的大模型提前拥有解决本身难以解决的复杂问题的能力  
  
这一部分也就是Agent 他需要完成Planning来帮助他解决这个任务  

而planning的结果未必就符合我们的预期，所以我们还需要Feedback反馈给我们结果，然后根据反馈去调整我们的解决问题的方法
  
所也有一部分设计者们把Feedback也纳入了Planning的范畴之内，也就有这样的范式  


> Agent = LLMS + Memory + Planning + Tools   



## 工具与工具使用

上面的内容我们聊过了，通过合理地使用工具，agent 就可以做到很多超出自己自身能力的事情

这点与人类也相同，**工具的使用是人类的一个显着而显着的特征。我们创造、修改和利用外部物体来完成超出我们身体和认知极限的事情**。为 LLM 配备外部工具可以扩展模型功能。比如我们其实没有那么擅长进行大规模的计算，但我们善于利用各种大型计算器来减少我们自身的计算负担，这种特性也是人类的智慧之一

那要如何利用大模型来实现工具调用呢？

事实上相当简单，具体的实现代码同学们可以参考 datawhale 另一开源项目 [tiny-universe](https://github.com/datawhalechina/tiny-universe/tree/main/content/TinyAgent)，对于大模型而言，计算可能确实困难，但对于理解如何调用工具就相当简单，只需要提供合适的提示词，接着将大模型的给我们的答案做一个解析，交给我们的工具api即可，最后将工具得到的结果返回，或者交由模型润色后返回即可

事实上简单的提示词工程就能做到

```python

"""
## Tools
xxx api ：用于xxx
调用 该api的数据格式如下：
{
    xxx:xxx,
    XXX:xxx
}
## Task
请你判断用户需求是否需要调用 Tools 中的工具
如果需要请以 json 格式返回工具调用 api 相关参数，不要输出任何无关内容
如果不需要，则返回字符 null
"""

import re
def api_call(response_from_llm : str):
    pattern = r'```json(.*)```'
    try:
        match = re.search(pattern, response_from_llm, re.DOTALL)
        ... ## 根据参数调用api
    except:
        if response_from_llm == "null":
            ....
        else:
            print("error") ## 或者将错误反馈给llm重新生成
```

工具的讲解就到此为止，虽然简单，但这将成为我们构建 Dify 工作流的重要核心

